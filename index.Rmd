---
title: "Workflow Management Systems"
subtitle: "reproducible workflows in R, Python, and beyond"
date: '`r Sys.Date()`'
output:
  xaringan::moon_reader:
    self_contained: true
    css: ['xaringan-themer.css', 'extra.css']
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 16:9
      countIncrementalSlides: true
   
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3, 
  out.width = "100%",
  cache = FALSE,
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  fig.show = TRUE,
  hiline = TRUE
)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#0065BD",
  secondary_color = "005293",
  inverse_header_color = "#FFFFFF"
)


xaringanExtra::use_editable()
xaringanExtra::use_tachyons()
xaringanExtra::use_panelset()
xaringanExtra::use_extra_styles()
```

## A "standard" workflow

- Idealized representation of a data-science type workflow ¹
- Starts with some set of raw data and ideally ends with communicable results  
- Your mileage may vary  


```{r echo = FALSE, fig.align='center', fig.retina=2, out.width = "70%"}
knitr::include_graphics("img/data-science.png")
```
.footnote[[1]https://r4ds.had.co.nz/explore-intro.html]

---
## The Myth of Sisyphus


```{r echo = FALSE, fig.align='center', fig.retina=2, out.width = "50%"}
knitr::include_graphics("img/data-science-explore.png")
```

```{r echo = FALSE, fig.align='center', fig.retina=2, out.width = "30%"}
knitr::include_graphics("img/Sisyphus-Overcoming-Silhouette.svg")
```

.footnote[[1] https://r4ds.had.co.nz/explore-intro.html, [2] https://wlandau.github.io/rpharma2020/]
---

## Potential Pitfalls

- Over successive iterations we lose track of changes we make
- We may have to devise a way to store or cache computationally expensive steps
- Can be difficult to come back to/get reacquainted with after some time
- Interactive session pollutes workspace
- Less than reproducible

.pull-left[.can-edit[
```{r import_data, eval=FALSE}
## Script to import the data
library(tidyverse)
climate_data <- 
  brick("data/CRUNCEP_EU_1901_2017.nc") %>%  
  rasterToPoints()
model_output <- 
  read_csv("data/cmass.out")
```

```{r model, eval=FALSE}
## Script to fit linear regressions
lin_reg_beech <- 
  lm(Fag_syl ~ precip, data_roi)

smr_lm <- summary(lin_reg_beech)

```

]]

.pull-right[.can-edit[

```{r wrangle, eval=FALSE}
## Script to wrangle data
source("import_data.R")

cmass_beech <- 
  select(c(Year, Lon, Lat, Fag_syl))
data_roi <- 
  left_join(cmass_beech, model_output)
```


```{r source_confusion, eval=FALSE}
## Script to visualize results
library(raster)
library(ggplot2)
library(lubridate)
source("build_model.R")
```
]]

---

## Notebooks

- RMarkdown, Quarto, Jupyter, etc.

- Offers some solutions: caching, forces linear structure, intertwined documentation


```{r markdown_img, echo=FALSE, out.extra='style="border-style: solid; border-color: #0065BD; border-radius: 3px; margin-left: auto; margin-right: auto; display: block"', out.width='60%', fig.align='right'}
knitr::include_graphics("img/markdown_example.png")
```

.center[
##**Does not always scale well!**
]

---
class:center middle
background-color:#0065BD

<h1 style=color:#FFFFFF> Workflow Management Systems </h1>

---

## What are they?

--

**In general**
--
.bg-near-white.b--dark-blue.ba.bw1.br2[
>**Infrastructure designed to facilitate organization, execution, and monitoring of a set of variably interdependent tasks**
]
--  

**In this case**

--
.bg-near-white.b--dark-blue.ba.bw1.br2[
>**Software to facilitate automated, reproducible, sustainable, and scalable data analysis processes**
]

---
## How do they work?

- workflows are decomposed into single steps
  - often function based

- each step generally takes an input and creates an output
  - each step has a singular task without side-effects

- dependencies between steps are monitored
  - usually as a directed acyclic graph
  - unidirectional

- status of each step is monitored
  - steps are only re-run if changes are detected
  - including upstream changes

**The software takes care of the overhead**

---
class:center middle
background-color:#0065BD
<h1 style=color:#FFFFFF><a href="https://docs.ropensci.org/targets/">.white[{targets}]</a></h1>


---

## What is {targets}?
.bg-near-white.b--dark-blue.ba.bw1.br2[
>The `targets` package is a **Make-like pipeline toolkit** for statistics and data science in R. With targets, you can maintain a **reproducible workflow without repeating yourself**. `targets` **skips costly runtime** for tasks that are already up to date, runs the necessary computation with implicit parallel computing, and abstracts files as R objects. A fully up-to-date targets pipeline is **tangible evidence that the output aligns with the code and data**, which substantiates **trust in the results**.¹
]

### Key concepts

- analyses are structured as pipelines
- each intermediate step of a pipeline is a `'target'`
- each `'target'` is immutable 
- function-oriented programming style
- data / output files are automatically managed

.footnote[[1] https://docs.ropensci.org/targets/]

---

## Getting started
.panelset[.panel[.panel-name[Install and set up]
.pull-left[
- target script file *(`_targets.R`)* is required
- the rest of the file structure is largely up to the user
- Lends itself to use with R Projects in RStudio
]

.pull-right[.can-edit[
```{r, eval = FALSE}
├── _targets.R
├── R/
├──── functions.R
├── data/
└──── raw_data.csv
```
]]

```{r new_targets, eval=FALSE}
## From CRAN
install.packages("targets")
targets::tar_script() ## creates new target script in current working directory
targets::tar_edit() ## opens existing target script in current working directory
```

]
.panel[.panel-name[Target script skeleton]
.can-edit[
```{r ex_script, eval=FALSE}
library(targets)
# This is an example _targets.R file. Every
# {targets} pipeline needs one.
# Use tar_script() to create _targets.R and tar_edit()
# to open it again for editing.
# Then, run tar_make() to run the pipeline
# and tar_read(summary) to view the results.

# Define custom functions and other global objects.
# This is where you write source(\"R/functions.R\")
# if you keep your functions in external scripts.
summ <- function(dataset) {
  summarize(dataset, mean_x = mean(x))
}
# Set target-specific options such as packages.
tar_option_set(packages = "dplyr")
## End this file with a list of target objects.
list(
  tar_target(data, data.frame(x = sample.int(100), y = sample.int(100))),
  tar_target(summary, summ(data)) # Call your custom functions as needed.
)
```
]
]]
---

## Where to start?

**...and what to keep where**

--


.pull-left.b--dark-blue.ba.bw1.br2[
- use the _targets.R script as a high-level plan for the analysis workflow
  - each target calls a function
  - each important intermediate step becomes a target
  - isolate computationally costly steps

```{r tar, eval=FALSE}
tar_plan(
  tar_target(
    EU25_DEM,
    "eu_dem_v11_E40N20.nc",
    format = "file"
  ),
  tar_target(
    EU25_DEM_WGS84,
    dem_to_WGS84(EU25_DEM),
    format = "file"
  )
)
```
]

--
.pull-right.b--dark-blue.ba.bw1.br2[
- functions are kept in separate file(s) away from the _targets.R script
  - functions do the heavy lifting
  - matching function names and target names creates a tangible link between the abstracted plan and the actual code

```{r fun, eval=FALSE}
dem_to_WGS84 <- function(EU25_DEM) {
  gdalUtils::gdalwarp(
    EU25_DEM, tmp_name,
    t_srs = "EPSG:4326"
  )
  gdalUtils::gdal_translate(
    tmp_name, out_name, 
    of = "netCDF"
  )
  
  return(out_name)
}
```
]
---

## Why even bother?

- targets are cached once run
- {targets} keeps track of the dependencies between individual targets
- changes in functions or targets are are automatically detected and reflected in the dependency graph
.pull-left[
```{r, echo=FALSE}
knitr::include_graphics("img/tar_vis.png")
```
]
.pull-right[
```{r, echo=FALSE}
knitr::include_graphics("img/tar_vis_out.png")
```
]

---

## How it works

####**1. Run the pipeline**

*`tar_make()`*
- run all (outdated) targets or specify which targets to run

####**2. Load targets to inspect**
*`tar_load()`*
- loads return value of the desired targets into the current environment
- loaded objects can be manipulated without affecting the pipeline and cache

####**3. Visualize pipeline**
*`tar_visnetwork()`*
- returns a dependency graph of the pipeline 
- can display metadata for each target

####**4. Rinse and repeat**

---

## Building targets

.pull-left[

**`tar_target()`**
- minimal target requires two arguments
- target name
- function call
]

.can-edit.pull-right[
```{r, eval=FALSE}
tar_target(
  name = first_target,
  command = first_function("path/to/data.nc")
)
```
]

--

.pull-left[
- additional arguments provide functionality for special cases
- `format` to specify how the output should be stored
  - can be used to keep track of files
- `cue` specifies when target should be (re)run
- `pattern` to specify iteration pattern
]

---

## What about R Markdown and Quarto

- still valuable for communicating results
- `tar_render()` provides integration for .Rmd documents
  - `tar_quarto()` for Quarto
- scaling and caching is handled by targets 
- code is largely isolated from markdown document to reduce clutter
  - only the last steps are done within markdown (i.E. plotting or simply loading targets)
  
.pull-left[
```{r, eval=FALSE}
   tar_target(
       npp_franconia,
       crop_npp(npp_raw, shp_franconia)
   ),
   
   tar_target(
       npp_by_county,
       summarize_npp(npp_franconia)
   ),
  ...
   tar_render(slides, "doc/slides.Rmd")
   tar_quarto(slides, "doc/slides.qmd")

```
]
.pull-right[
```{r, echo=FALSE}
knitr::include_graphics("img/tar_rmd.png")
```
]

---

## Further reading and useful links

Miles McBain: 
- **https://www.youtube.com/watch?v=jU1Zv21GvT4**
- **https://www.milesmcbain.com/posts/the-drake-post/**

*These cover drake, the predecessor to targets, but the concepts remain the same*

- **https://github.com/MilesMcBain/fnmate**
- **https://github.com/MilesMcBain/tflow**

*Two R packages which provide useful addins to be used with targets*

Will Landau: 
- **https://www.youtube.com/watch?v=Gqn7Xn4d5NI**
- **https://wlandau.github.io/posts/2020-12-14-targetopia/**
- **https://books.ropensci.org/targets/**

*Straight from the source; Will Landau is the author of targets*

---
class:center middle
background-color:#0065BD

<h1 style=color:#FFFFFF><a href="https://snakemake.github.io/"> .white[snakemake]</a></h1>

---

## What is snakemake?

.bg-near-white.b--dark-blue.ba.bw1.br2[
>The Snakemake workflow management system is a tool to create **reproducible and scalable data analyses**. Workflows are described via a **human readable, Python based language**. They can be seamlessly scaled to server, cluster, grid and cloud environments, **without the need to modify** the workflow definition. Finally, Snakemake **workflows can entail a description of required software**, which will be automatically deployed to any execution environment.¹
]

### Key concepts

- workflows are decomposed into rules
  - instruction how to create a set of output file from a set of input files
- wildcards can be used to generalize
- software dependencies can be managed via e.g. conda
- not tied to a specific language
  - steps can call scripts do work

.footnote[[1] https://snakemake.readthedocs.io/en/stable/]

---

## Getting started

.panelset[.panel[.panel-name[Install and set up]
.pull-left[
- a file (`snakefile`) detailing the rules is required 
- the rest of the file structure is largely up to the user
- Snakemake authors suggest using VSCode since it supports syntax highlighting for snakemake
]

.pull-right[.can-edit[
```{bash, eval = FALSE}
├── snakefile
├── scripts/
├──── script1.R
├──── script2.py 
├── data/
└──── raw_data.csv
```
]]

```{bash, eval=FALSE}
### In terminal or terminal integrated in your favorite IDE
conda create -c conda-forge -c bioconda -n snakemake snakemake 
conda activate snakemake 
snakemake --help
```

]
.panel[.panel-name[Snakefile]
.can-edit[
```{bash example, eval=FALSE}

rule clean_data:
    input:
        "data/raw_data.csv",
    output:
        "data/clean_data.csv"
    script:
        "script1.R"

rule visualize_data:
    input:
        "data/clean_data.csv",
    output:
        "plots/plot1.svg"
    script:
        "script2.py"

```
]
.can-edit[
```{bash execute_snake, eval=FALSE}
snakemake 
```
]
]]

---

## Where to start?

**...and what to keep where**

--


.pull-left.b--dark-blue.ba.bw1.br2[
- use the snakefile script as a high-level plan for the analysis workflow
  - each rule executes a single script or command
  - each important intermediate step becomes a rule
  - isolate computationally costly steps

```{bash snfl, eval=FALSE}

rule EU25_DEM_WGS84:
    input:
        "eu_dem_v11_E40N20.nc"
    output:
        "eu_dem_v11_E40N20_WGS84.nc"
    script:
        "dem_to_WGS84.R"
```
]

--
.pull-right.b--dark-blue.ba.bw1.br2[
- functions are kept in separate files away from the snakefile script
  - functions do the heavy lifting
  - matching function/script names and rule names creates a tangible link between the abstracted plan and the actual code

```{r fun2, eval=FALSE}
  gdalUtils::gdalwarp(
    snakemake@input[[1]], "tmp_file.nc",
    t_srs = "EPSG:4326"
  )
  gdalUtils::gdal_translate(
    "tmp_file.nc", snakemake@output[[1]], 
    of = "netCDF"
  )
}
```
]

--- 

## Why even bother?

---

## How it works

**1. Run the pipeline**

.blue[*`snakemake -n`*]
  - dry-run
  - which rules will be run and how long will it take?

.blue[*`snakemake`*]
  - run all outdated rules
  
**2. Load outputs to inspect**

  - all outputs are files
  - read, view, etc. like regular files
  
**3. Visualize pipeline**

  .blue[*`snakemake --dag | dot -Tsvg > dag.svg`*]
  
  - returns a dependency graph
  - for clarity in large pipelines you can specify individual steps

**4. Rinse and repeat**

---

## What is the right tool for you?

**targets**
- your preferred language is R
- inputs to your analyses are not part of your pipeline
  - observational data
  - data from 3rd parties
- you are tied to RMarkdown

**snakemake**
- you either:
  - do not use R
  - want to use multiple languages
- part of your pipeline includes running external models
  - e.g. LPJ-GUESS
- you want to include automatic downloads or similar tasks

---

## Further reading and useful links

- **https://snakemake.readthedocs.io/en/stable/**
- **https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html#tutorial**

*Official, extensive documentation including a tutorial*

- **https://f1000research.com/articles/10-33/v2**

*Peer-reviewed rolling paper*

- **https://snakemake.github.io/snakemake-workflow-catalog/**

*Catalog of fully developed real-world example following best practices*

---


## What's next?

1. Pick a project that you are working on

2. Start converting to targets/snakemake

3. In the next session we will can troubleshoot, debug, and more